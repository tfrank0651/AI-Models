{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of CS3793_5233_assignment3_zaf455.ipynb","provenance":[{"file_id":"1VXQ79QmfLw4JUtgaN2M3EWzx3IHN6bAm","timestamp":1660253898352}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"9wUL_Fy5qUDI"},"source":["# UTSA CS 3793/5233: Assignment-3\n","\n","Summer 2021\n","\n","\n","**Frank - Tyler - (zaf455)**\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"NM8b9KVYsETT"},"source":["## Learning Objectives\n","\n","Implement 2 different machine learning algorithms\n","*   Stochastic Gradient Descent\n","*   ID3 Decision Tree\n"]},{"cell_type":"markdown","metadata":{"id":"LzR4Ic34zJlT"},"source":["\n","## Description\n","\n","This assignment is focused on **machine learning**, mainly on the implementation of 2 different algorithms - Stochastic Gradient Descent & ID3 decision tree. \n","The assignment is divided into two sections, each for one unique ML algorithm. \n","\n","The base structure and comments are provided on what should be done. You can use some libraries that help support you for the successful completion of the assignment. However, you **CANNOT** use a complete library that contains the implementation of ML algorithms. You can get pieces of code from online, but please cite the source properly.\n"]},{"cell_type":"markdown","metadata":{"id":"vnPfmHAOteOI"},"source":["##Import Libraries\n","\n","Write all the import statements here. This should be for both algorithm implmentations. As mentioned before, you can not use any premade ML libraries."]},{"cell_type":"code","metadata":{"id":"f9apbZGptej6","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b34bf89a-ce09-421e-be74-c7be08d194b7"},"source":["# import all required libraries\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from numpy import dot\n","import math\n","from sklearn.metrics import mean_squared_error\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import operator\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fdqXyFZ95P0j"},"source":["# Assume that the data files are in the following folder -- THIS WILL BE USED BY THE TA\n","basePath = \"/content/drive/My Drive/Colab Notebooks/Artificial Intelligence/Data/\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YeYRnesWqvLm"},"source":["#Stochastic Gradient Descent\n","\n","In this section, you will implement the Stochastic Gradient Descent algorithm. The training is for a **binary classification** task i.e. each instance will have a class value of 0 or 1. Also, assume that you are given **all binary-valued attributes** and that there are **no missing values** in the train or test data. \n"]},{"cell_type":"markdown","metadata":{"id":"IUVZIK6ctMi4"},"source":["##Algorithm\n","\n","(50 points)\n","\n","Following are the data files that will be provided to you for the gradient descent algorithm implementation.\n","\n","*   Training file - 'gd-train.dat'\n","*   Testing file - 'gd-test.dat'\n","\n","*Both these files should be present in the same folder as this code file.* In these files, only non-space characters are relevant. The first line contains the attribute names. All the other lines are different example instances to be used for the algorithm. Each column holds values of the attributes, whereas the last column holds the class label for that instance.\n","\n","Write the code in the following code block, structure is provided. Instructions on the steps to follow are provided as comments.\n","\n"]},{"cell_type":"code","metadata":{"id":"2XoSqVJG5FkG"},"source":["# Data file name variables\n","train = basePath + \"gd-train.dat\"\n","test = basePath + \"gd-test.dat\"\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l73PTZtCPdxj","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1f271abe-4905-483e-85b8-5c74cdaa4709"},"source":["# Read the training and testing data files\n","# df_train = pd.read_csv(train)\n","# df_test = pd.read_csv(test)\n","df_train = pd.read_table(train)\n","df_test= pd.read_table(test)\n","\n","print('Training Cols')\n","# print(df_train.columns)\n","print(df_train.head(10))\n","print('\\nTesting Cols')\n","print(df_test.head(10))\n","\n","# rs = df_train.sample(100)\n","# f, ax = plt.subplots(figsize = (15,10))\n","# plt.rc('font', size=18)\n","# ax.set_xlabel(\"A1\")\n","# ax.set_ylabel(\"C\")\n","# ax.xaxis.label.set_color('black')\n","# ax.yaxis.label.set_color('black')\n","# ax.tick_params(colors='black')\n","# plt.scatter(x=rs.A1, y=rs.C, color='blue')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Training Cols\n","   A1  A2  A3  A4  A5  A6  A7  A8  A9  A10  A11  A12  A13  C\n","0   1   1   1   1   0   0   1   1   0    0    0    1    1  0\n","1   0   0   0   1   0   0   1   1   0    1    0    0    1  0\n","2   0   1   1   1   0   1   1   1   1    0    0    0    1  0\n","3   0   1   1   0   1   0   1   1   1    0    1    0    1  0\n","4   0   1   0   0   0   1   0   1   0    1    0    0    1  0\n","5   0   1   1   0   0   1   1   1   1    1    1    0    1  0\n","6   0   1   1   1   0   0   1   1   0    0    0    1    1  0\n","7   0   1   0   0   1   0   0   1   1    0    1    1    1  0\n","8   1   1   1   1   0   0   1   1   0    0    0    0    1  0\n","9   1   0   1   1   1   1   1   0   0    0    0    0    1  0\n","\n","Testing Cols\n","   A1  A2  A3  A4  A5  A6  A7  A8  A9  A10  A11  A12  A13  C\n","0   1   1   0   0   0   0   0   0   1    1    0    0    1  0\n","1   0   0   1   1   0   1   1   0   0    0    0    0    1  0\n","2   0   1   0   1   1   0   1   0   1    1    1    0    1  1\n","3   0   0   1   0   0   1   0   1   0    1    1    1    1  0\n","4   0   1   0   0   0   0   0   1   1    1    1    1    1  0\n","5   0   1   1   1   0   0   0   1   0    1    1    0    1  1\n","6   0   1   1   0   0   0   1   0   0    0    0    0    1  0\n","7   0   0   0   1   1   0   1   1   1    0    0    0    1  0\n","8   0   0   0   0   0   0   1   0   1    0    1    0    1  0\n","9   1   0   1   1   1   0   0   0   1    1    1    0    1  1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VN3FC2qcPfcR"},"source":["# Activation Function - implement Sigmoid\n","# S(z) = 1.0 / (1 + (e**(-z)))\n","# Source: https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html\n","\n","def activation_function(h):\n","    # given 'h' compute and return 'z' based on the activation function implemented\n","    z = 1.0 / (1 + np.exp(-h))\n","    # print('activation_function returning: ' + str(z)) #-debug\n","    return z"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V5njAQvmPmOC"},"source":["# Train the model using the given training dataset and the learning rate\n","# return the \"weights\" learnt for the perceptron - include the weight assocaited with bias as the last entry\n","# def train(train_data, learning_rate=0.05):\n","def train(train_data, learning_rate):\n","    countCorrect = 0\n","    # initialize weights to 0\n","    weights = [0] * 13\n","    b = weights[0]\n","    # print(weights) #-debug\n","    # print(str(len(train_data)))#-debug\n","    count = 0 #-debug\n","    # go through each training data instance\n","    for index, x in df_train.iterrows():\n","        # get 'x' as one multi-variate data instance and 'y' as the ground truth class label\n","        # print('index: ' + str(index))#-debug\n","        # print('x: ' + str(x))#-debug\n","        y = x[13]\n","        # print(str(mvX) + ' gtcl:' + str(gtcl)) #-debug\n","        # obtain h(x)\n","        ## h(x) = (w[0]*x[0] + w[1]*x[1] + ... + w[12]*x[12]) + b\n","        h = 0\n","        idx = 0\n","        while idx < 13:\n","            debug1 = x[idx]\n","            h += weights[idx] * x[idx]\n","            # print(h)\n","            idx += 1\n","        # h += b # ignoring for now\n","        # print('h:' + str(h)) #-debug\n","        # call the activation function with 'h' as parameter to obtain 'z'\n","        # print('h:' + str(h)) #-debug\n","        z = activation_function(h)\n","        classification = GetClassification(z)\n","        if classification == y:\n","            countCorrect += 1\n","        # print('z: ' + str(z)) #-debug\n","        # update all weights individually using learning_rate, (y-z), and the corresponding 'x'\n","        j = 0\n","        while j < 13:\n","            # print('w[j]:' + str(weights[j]) + ' + learning rate:' + str(learning_rate) + '* (y-z): ' + str(y-z) + ' * x[j]: ' + str(x[j]))#-debug\n","            weights[j] = weights[j] + learning_rate * (y - z) * x[j]\n","\n","            # print(weights) #-debug\n","            j += 1\n","        # print() #-debug\n","        count += 1 #-debug\n","    # return the final learnt weights\n","    acc = countCorrect / len(train_data)\n","    # print('returning weights: ' + str(weights))\n","    return weights, acc  ## need to add weight associated with bias as last entry"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5IpKwtuX6nkO"},"source":["# Best way to iterate through dataframes in pandas - for future study\n","# https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas\n","# https://engineering.upside.com/a-beginners-guide-to-optimizing-pandas-code-for-speed-c09ef2c6a4d6"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gx98kNxDPq5B"},"source":["# Test the model (weights learnt) using the given test dataset\n","# return the accuracy value\n","def test(test_data, weights, threshold):\n","    countCorrect = 0\n","    # go through each training data instance\n","    for index, x in test_data.iterrows():\n","        # get 'x' as one multi-variate data instance and 'y' as the ground truth class label\n","        y = x[13]\n","        # obtain h(x)\n","        h = 0\n","        idx = 0\n","        while idx < 13:\n","            h += weights[idx] * x[idx]\n","            # print(h)\n","            idx += 1\n","        # call the activation function with 'h' as parameter to obtain 'z'\n","        z = activation_function(h)\n","        # use 'threshold' to convert 'z' to either 0 or 1 so as to match to the ground truth binary labels\n","        classification = GetClassification(z)\n","        # compare the normalized 'z' with 'y' to calculate the positive and negative instances for calculating accuracy\n","        if classification == y:\n","            countCorrect += 1\n","    # return the accuracy value for the given test dataset\n","    accuracyVal = countCorrect / len(test_data)\n","    return accuracyVal"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_5B6IG0APxH4"},"source":["# Gradient Descent function\n","def gradient_descent(df_train, df_test, learning_rate, threshold=0.5):\n","    # call the train function to train the model and obtain the weights\n","    learntWeights, accuracyValTrain = train(df_train, learning_rate)\n","    accuracyValTrain = accuracyValTest = test(df_train, learntWeights, 0.5)\n","    # call the test function with the training dataset to obtain the training accuracy\n","    # accuracyValTrain = test(train, learntWeights, 0.5)\n","    # call the test function with the testing dataset to obtain the testing accuracy\n","    accuracyValTest = test(df_test, learntWeights, 0.5)\n","    # return (trainAccuracy, testAccuracy)\n","    return (accuracyValTrain, accuracyValTest)\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MB_b5ButP3w3"},"source":["# Threshold of 0.5 will be used to classify the instance for the test. If the value is >= 0.5, classify as 1 or else 0.\n","threshold = 0.5\n","def GetClassification(value):\n","  if value >= 0.5:\n","    return 1\n","  if value < 0.5:\n","    return 0\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YdPwgSBOtb1P","colab":{"base_uri":"https://localhost:8080/"},"outputId":"92e321d3-c230-455e-e1a3-a5e7dc98fe4a"},"source":["# Main algorithm loop\n","# Loop through all the different learning rates [0.05, 1]\n","    # For each learning rate selected, call the gradient descent function to obtain the train and test accuracy values\n","    # Print both the accuracy values as \"Accuracy for LR of 0.1 on Training set = x %\" OR \"Accuracy for LR of 0.1 on Testing set = x %\"\n","# learnRates = [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1]\n","plotX = []\n","trainAccY = []\n","testAccY = []\n","learnRates = [0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1]\n","for lr in learnRates:\n","  accTrain, accTest = gradient_descent(df_train, df_test, lr)\n","  plotX.append(lr)\n","  testAccY.append(accTest)\n","  trainAccY.append(accTrain)\n","  print('Accurracy for LR of ' + str(lr) + ' on Training set = '+ str(accTrain * 100) + '%')\n","  print('Accurracy for LR of ' + str(lr) + ' on Testing set = '+ str(accTest * 100) + '%')\n","  print()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Accurracy for LR of 0.05 on Training set = 68.0%\n","Accurracy for LR of 0.05 on Testing set = 72.25%\n","\n","Accurracy for LR of 0.1 on Training set = 68.0%\n","Accurracy for LR of 0.1 on Testing set = 71.75%\n","\n","Accurracy for LR of 0.15 on Training set = 69.0%\n","Accurracy for LR of 0.15 on Testing set = 71.75%\n","\n","Accurracy for LR of 0.2 on Training set = 69.0%\n","Accurracy for LR of 0.2 on Testing set = 71.5%\n","\n","Accurracy for LR of 0.25 on Training set = 69.0%\n","Accurracy for LR of 0.25 on Testing set = 71.0%\n","\n","Accurracy for LR of 0.3 on Training set = 69.0%\n","Accurracy for LR of 0.3 on Testing set = 70.75%\n","\n","Accurracy for LR of 0.35 on Training set = 70.0%\n","Accurracy for LR of 0.35 on Testing set = 70.0%\n","\n","Accurracy for LR of 0.4 on Training set = 70.0%\n","Accurracy for LR of 0.4 on Testing set = 69.0%\n","\n","Accurracy for LR of 0.45 on Training set = 69.0%\n","Accurracy for LR of 0.45 on Testing set = 68.5%\n","\n","Accurracy for LR of 0.5 on Training set = 69.0%\n","Accurracy for LR of 0.5 on Testing set = 68.25%\n","\n","Accurracy for LR of 0.55 on Training set = 69.0%\n","Accurracy for LR of 0.55 on Testing set = 67.5%\n","\n","Accurracy for LR of 0.6 on Training set = 69.0%\n","Accurracy for LR of 0.6 on Testing set = 66.25%\n","\n","Accurracy for LR of 0.65 on Training set = 69.0%\n","Accurracy for LR of 0.65 on Testing set = 65.75%\n","\n","Accurracy for LR of 0.7 on Training set = 69.0%\n","Accurracy for LR of 0.7 on Testing set = 65.0%\n","\n","Accurracy for LR of 0.75 on Training set = 68.0%\n","Accurracy for LR of 0.75 on Testing set = 65.0%\n","\n","Accurracy for LR of 0.8 on Training set = 67.0%\n","Accurracy for LR of 0.8 on Testing set = 65.0%\n","\n","Accurracy for LR of 0.85 on Training set = 66.0%\n","Accurracy for LR of 0.85 on Testing set = 64.5%\n","\n","Accurracy for LR of 0.9 on Training set = 66.0%\n","Accurracy for LR of 0.9 on Testing set = 64.25%\n","\n","Accurracy for LR of 0.95 on Training set = 67.0%\n","Accurracy for LR of 0.95 on Testing set = 64.5%\n","\n","Accurracy for LR of 1 on Training set = 68.0%\n","Accurracy for LR of 1 on Testing set = 64.5%\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lYxmgcnes9cS"},"source":["##Extra Credit - Accuracy Plots\n","\n","(05 points)\n","\n","Use the above accuracy results on the training and testing data and write code to plot the graphs as mentioned in the code block below.\n","\n"]},{"cell_type":"code","metadata":{"id":"fbBNakSDq0Wv","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"8e30c01f-9971-47e8-9d4c-1694f33dbbc9"},"source":["# Plot the graphs for accuracy results.\n","# plotX = []\n","# trainAccY = []\n","# testAccY = []\n","# There will be 2 graphs - one for training data and the other for testing data\n","# For each graph,\n","    # X-axis will be the learning rate going from 0.05-1 in increments on 0.05\n","    # Y-axis will be the accuracy values at the selected learning rate.\n","plt.rcParams[\"figure.figsize\"] = (15, 10)\n","plt.scatter(plotX, trainAccY, c='lightblue')\n","plt.xlabel('Training Data Accuracy')\n","plt.ylabel('Learning Rate')\n","plt.title('Training Accuracy')\n","plt.axis([0.05, 1, 0.5, 0.8])\n","plt.xticks([0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1])\n","plt.show()\n","\n","plt.scatter(plotX, testAccY, c='coral')\n","plt.xlabel('Test Data Accuracy')\n","plt.ylabel('Learning Rate')\n","plt.title('Test Accuracy')\n","plt.axis([0.05, 1, 0.5, 0.8])\n","plt.xticks([0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1])\n","plt.show()\n"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAA4kAAAJcCAYAAABUquF9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdfbxld10f+s83MwUSDCGQgN48QKDBSJASOWIVq6g8pFqICtcbtBXqAz6h91qvXnzVXmnUXq+3VmqbKsFSkQrR0lscWykFNSIIkokJ6EQDScA8qDFPjIGEh0m+/WOv+bkznnPmTGb27H1m3u/X67xmr99aa+/PWdln5XzOetjV3QEAAIAkOWHZAQAAAFgdSiIAAACDkggAAMCgJAIAADAoiQAAAAxKIgAAAIOSCMC2V1Vvq6qXHellAeB4VD4nEYBlqKqPz02elORTSe6fpr+ju3/56Kc6fFV1TpIbkry2u79r2XkA4FA5kgjAUnT3Z+3/SnJTkhfOjY2CWFU7l5fyIfnmJHcn+d+q6uFH84WrasfRfD0Ajk1KIgArpaqeU1W3VNX/VVV/keQ/VNWpVfVfq+r2qrp7enzm3DpXVNW3TY9fXlXvrqp/OS37kar6+w9x2XOq6l1VdU9VvbOqLq2q/7hJ9sqsJP5Iks8keeEB8y+qqmuq6q+q6oaqunAaf0xV/Yeq+rMpx1vn8x3wHF1Vf3t6/ItV9XNV9RtV9YkkX1FVX1NVV0+vcXNVvfqA9b+0qn6vqj42zX95VX1hVd02XzKr6uur6gNb+o8GwDFFSQRgFX12ksckeUKSV2T2/6v/ME2fneS+JP92k/W/KMl1SU5L8lNJ/v1U4A512TcleX+SxyZ5dZJ/dJDcX5rkzCSXJ/nVJOPax6p6VpJfSvKDSR6d5MuSfHSa/cbMTrk9P8njkvzMQV5n3jcm+YkkJyd5d5JPZFZUH53ka5J8V1V97ZThCUneluTfJDk9yTOSXNPdVya5M8nz5573H015ATjObLdTeAA4PjyQ5Ee7+1PT9H1J/vP+mVX1E0l+e5P1/7S7Xzct+4Yk/y7J45P8xVaXraqHJfnCJF/V3Z9O8u6q2nWQ3C9L8rbuvruq3pTkXVX1uO7+yyTfmuT13f2Oadlbp9f8nCR/P8lju/vuad7vHOR15v1ad79nevzJJFfMzftgVb05yZcneWtmhfKd3f3maf6d01eSvCHJP0zytqp6TJIXJPnuQ8gBwDHCkUQAVtHt3f3J/RNVdVJVvbaq/rSq/irJu5I8epNr8EYZ7O57p4efdYjL/i9J7pobS5KbNwpcVScm+V+T/PL0XO/N7FrLb5wWOSuzG9oc6Kzpde5eZ95WPChTVX1RVf32dGru3iTfmdlR0s0yJMl/TPLCqnpkkm9I8rvd/ecPMRMA25iSCMAqOvDW2z+Q5HOTfFF3PyqzUzWTZKNTSI+EP0/ymKo6aW7srE2W/7okj0ry76rqL6brKc/IX59yenOSJ6+z3s3T6zx6nXmfyOw01CRJVX32OsscuK3elGRXkrO6+5QkP5+/3k4bZUh335rkvUm+PrNTTd+43nIAHPuURAC2g5MzO+X0Y9OpkD+66Bfs7j9NsjvJq6vqYVX1xTngRjQHeFmS1yf5/Myu9XtGkmcn+TtV9flJ/n2Sf1xVX1VVJ1TVGVV13nS07m2ZlctTq+pvVdX+EvyBJOdX1TOq6hGZXRd5MCdndmTyk9N1kN84N++Xkzy3qr6hqnZW1WOr6hlz838pyQ9N38P/v4XXAuAYpCQCsB28JsmJSe5I8r4k//0ove43JfnizK7b+/Ekv5LZ5zk+SFWdkeSrkrymu/9i7uuqKevLuvv9Sf5xZjel2ZvZdYdPmJ7iH2V2N9Q/SfKXSf6PJOnuDyW5JMk7k3w4sxvTHMx3J7mkqu5J8n9ndgOdTM93U5KvzuzI7F1Jrknyd+bW/S9Tpv9ywGm2ABxHqvvAs1QAgPVU1a8k+ZPuXviRzGWpqhuSfEd3v3PZWQBYDkcSAWAD0+cHPnk6PfTCJBdldpfQY1JVvTizaxx/a9lZAFiehZbEqrqwqq6rquur6lXrzD97ugPb1VX1war66rl5Pzytd11VvWCROQFgA5+d2UdKfDzJzyb5ru6+eqmJFqSqrkjyc0m+p7sfWHIcAJZoYaebTrcl/1CS5yW5JcmVSV7a3dfOLXNZkqu7++eq6qlJfqO7nzg9fnOSZ2V2C/J3JnlKd9+/kLAAAAAkWeyRxGclub67b5w+hPjyzE7TmdeZ3S48SU5J8mfT44uSXN7dn+rujyS5fno+AAAAFmjnAp/7jDz4A35vSfJFByzz6iT/o6q+N8kjkzx3bt33HbDuGQe+QFW9IskrkuSRj3zkM88777wjEhwAAGC7ueqqq+7o7tMP93kWWRK34qVJfrG7f3r6/Kk3VtXTtrpyd1+W5LIkWVtb6927dy8oJgAAwGqrqj89Es+zyJJ4a5Kz5qbPnMbmfWuSC5Oku987fVDwaVtcFwAAgCNskdckXpnk3Ko6p6oeluTiJLsOWOamzD58OFX1eUkekeT2abmLq+rhVXVOknOTvH+BWQEAAMgCjyR2976qemWStyfZkeT13b2nqi5Jsru7dyX5gSSvq6rvz+wmNi/v2e1W91TVrya5Nsm+zG7H7c6mAAAAC7awj8A42lyTCAAAHM+q6qruXjvc51nk6aYAAABsM0oiAAAAg5IIAADAoCQCAAAwKIkAAAAMSiIAAACDkggAAMCgJAIAADAoiQAAAAxKIgAAAIOSCAAAwKAkAgAAMCiJAAAADEoiAAAAg5IIAADAoCQCAAAwKIkAAAAMSiIAAACDkggAAMCgJAIAADAoiQAAAAxKIgAAAIOSCAAAwKAkAgAAMCiJAAAADEoiAAAAg5IIAADAoCQCAAAwKIkAAAAMSiIAAACDkggAAMCgJAIAADAoiQAAAAxKIgAAAIOSCAAAwKAkAgAAMCiJAAAADEoiAAAAg5IIAADAoCQCAAAwKIkAAAAMSiIAAACDkggAAMCgJAIAADAoiQAAAAxKIgAAAIOSCAAAwKAkAgAAMCiJAAAADEoiAAAAg5IIAADAoCQCAAAwKIkAAAAMSiIAAACDkggAAMCgJAIAADAoiQAAAAxKIgAAAIOSCAAAwKAkAgAAMCiJAAAADEoiAAAAg5IIAADAoCQCAAAwKIkAAAAMSiIAAACDkggAAMCgJAIAADAoiQAAAAxKIgAAAIOSCAAAwKAkAgAAMCiJAAAADEoiAAAAg5IIAADAoCQCAAAwKIkAAAAMSiIAAACDkggAAMCgJAIAADAoiQAAAAxKIgAAAMNCS2JVXVhV11XV9VX1qnXm/0xVXTN9faiqPjY37/65ebsWmRMAAICZnYt64qrakeTSJM9LckuSK6tqV3dfu3+Z7v7+ueW/N8kFc09xX3c/Y1H5AAAA+JsWeSTxWUmu7+4bu/vTSS5PctEmy780yZsXmAcAAICDWGRJPCPJzXPTt0xjf0NVPSHJOUl+a274EVW1u6reV1Vfu8F6r5iW2X377bcfqdwAAADHrVW5cc3FSd7S3ffPjT2hu9eSfGOS11TVkw9cqbsv6+617l47/fTTj1ZWAACAY9YiS+KtSc6amz5zGlvPxTngVNPuvnX698YkV+TB1ysCAACwAIssiVcmObeqzqmqh2VWBP/GXUqr6rwkpyZ579zYqVX18OnxaUmeneTaA9cFAADgyFrY3U27e19VvTLJ25PsSPL67t5TVZck2d3d+wvjxUku7+6eW/3zkry2qh7IrMj+5PxdUQEAAFiMenA3277W1tZ69+7dy44BAACwFFV11XRfl8OyKjeuAQAAYAUoiQAAAAxKIgAAAIOSCAAAwKAkAgAAMCiJAAAADEoiAAAAg5IIAADAoCQCAAAwKIkAAAAMSiIAAACDkggAAMCgJAIAADAoiQAAAAxKIgAAAIOSCAAAwKAkAgAAMCiJAAAADEoiAAAAg5IIAADAoCQCAAAwKIkAAAAMSiIAAACDkggAAMCgJAIAADAoiQAAAAxKIgAAAIOSCAAAwKAkAgAAMCiJAAAADEoiAAAAg5IIAADAoCQCAAAwKIkAAAAMSiIAAACDkggAAMCgJAIAADAoiQAAAAxKIgAAAIOSCAAAwKAkAgAAMCiJAAAADEoiAAAAg5IIAADAoCQCAAAwKIkAAAAMSiIAAACDkggAAMCgJAIAADAoiQAAAAxKIgAAAIOSCAAAwKAkAgAAMCiJAAAADEoiAAAAg5IIAADAoCQCAAAwKIkAAAAMSiIAAACDkggAAMCgJAIAADAoiQAAAAxKIgAAAIOSCAAAwKAkAgAAMCiJAAAADEoiAAAAg5IIAADAoCQCAAAwKIkAAAAMSiIAAACDkggAAMCgJAIAADAoiQAAAAxKIgAAAIOSCAAAwKAkAgAAMCiJAAAADEoiAAAAg5IIAADAoCQCAAAw7Fzkk1fVhUn+dZIdSX6hu3/ygPk/k+QrpsmTkjyuux89zXtZkh+Z5v14d79hkVmBY9NNe+/NnjvuyX37HsiJO0/I+aednLNPOWnZsTa03fICAMeehZXEqtqR5NIkz0tyS5Irq2pXd1+7f5nu/v655b83yQXT48ck+dEka0k6yVXTuncvKi9w7Llp7725+ra9ub9n0/fteyBX37Y3SVayeG23vADAsWmRp5s+K8n13X1jd386yeVJLtpk+ZcmefP0+AVJ3tHdd03F8B1JLlxgVuAYtOeOe0bh2u/+no2vou2WFwA4Ni2yJJ6R5Oa56Vumsb+hqp6Q5Jwkv3Uo61bVK6pqd1Xtvv32249IaODYcd++Bw5pfNm2W14A4Ni0KjeuuTjJW7r7/kNZqbsv6+617l47/fTTFxQN2K5O3Ln+Lm6j8WXbbnkBgGPTIn/zuDXJWXPTZ05j67k4f32q6aGuC7Cu8087OTvqwWM7aja+irZbXgDg2LTIknhlknOr6pyqelhmRXDXgQtV1XlJTk3y3rnhtyd5flWdWlWnJnn+NAawZWefclIuePwp40jciTtPyAWPP2VlbwKz3fICAMemhd3dtLv3VdUrMyt3O5K8vrv3VNUlSXZ39/7CeHGSy7u759a9q6p+LLOimSSXdPddi8oKHLvOPuWkbVWytlteAODYU3PdbFtbW1vr3bt3LzsGAADAUlTVVd29drjP424IAAAADEoiAAAAg5IIAADAoCQCAAAwKIkAAAAMSiIAAACDkggAAMCgJAIAADAoiQAAAAxKIgAAAMPOZQdg9d20997sueOe3LfvgZy484Scf9rJOfuUk5Yda0PyLtZ2y8tibbf3g7yLJe9ibbe8wPalJLKpm/bem6tv25v7ezZ9374HcvVte5NkJf/HJO9ibbe8LNZ2ez/Iu1jyLtZ2ywtsb043ZVN77rhn/A9pv/t7Nr6K5F2s7ZaXxdpu7wd5F0vexdpueYHtTUlkU/fte+CQxpdN3sXabnlZrO32fpB3seRdrO2WF9jelEQ2deLO9d8iG40vm7yLtd3ysljb7f0g72LJu1jbLS+wvdmzsKnzTzs5O+rBYztqNr6K5F2s7ZaXxdpu7wd5F0vexdpueYHtzY1r2NT+i+G3y93U5F2s7ZaXxdpu7wd5F0vexdpueYHtrbr74EttA2tra7179+5lxwAAAFiKqrqqu9cO93mcbgoAAMCgJAIAADAoiQAAAAxKIgAAAIOSCAAAwKAkAgAAMCiJAAAADEoiAAAAg5IIAADAoCQCAAAw7Fx2gCPlY5/8TN52w205/7STc/YpJy07zqZu2ntv9txxT+7b90BO3HnCtsgMAHAo/L4DR8/+n7cnnf/0Zx6J5ztmSmKS3LfvgVx9294kWdmd0E17783Vt+3N/T2b3g6ZAQAOhd934OiZ/3mrI/Scx9zppvd3sueOe5YdY0N77rhn7DD3W/XMAACHwu87cPSs9/N2uI65kpjM/lq1qjbKtsqZAQAOhd934OhZxM/VMVkST9y5ut/WRtlWOTMAwKHw+w4cPYv4uTrmflJ3VHL+aScvO8aGzj/t5Ow44GThVc8MAHAo/L4DR896P2+H65i6cc12uHPW/mzu9gUAHKv8vgNHz/zP25G6NLG6j/BVjkuytrbWu3fvXnYMAACApaiqq7p77XCf55g73RQAAICHTkkEAABgUBIBAAAYlEQAAAAGJREAAIBBSQQAAGBQEgEAABiURAAAAAYlEQAAgEFJBAAAYNi57AAAALBsN+29N3vuuCf37XsgJ+48IeefdnLOPuWkZcdiSY7394OSCADAce2mvffm6tv25v6eTd+374FcfdveJDmuigEz3g9ONwUA4Di35457RiHY7/6ejXP88X5QEgEAOM7dt++BQxrn2Ob9oCQCAHCcO3Hn+r8SbzTOsc37QUkEAOA4d/5pJ2dHPXhsR83GOf54P7hxDQAAx7n9NyM5nu9myV/zfthCSayqpyT5uSSP7+6nVdXTk7you3984ekAAOAoOPuUk46rEsDmjvf3w1ZON31dkh9O8pkk6e4PJrl4kaEAAABYjq2UxJO6+/0HjO1bRBgAAACWaysl8Y6qenKSTpKqekmSP19oKgAAAJZiKzeu+Z4klyU5r6puTfKRJN+00FQAAAAsxVZKYnf3c6vqkUlO6O57quqcRQcDAADg6NvK6ab/OUm6+xPdfc809pbFRQIAAGBZNjySWFXnJTk/ySlV9fVzsx6V5BGLDgYAAMDRt9nppp+b5B8keXSSF86N35Pk2xcZCgAAgOXYsCR2968l+bWq+uLufu9RzAQAAMCSbOXGNVdX1fdkdurpOM20u79lYakAAABYiq2UxDcm+ZMkL0hySWYff/HHiwwFAABs7Ka992bPHffkvn0P5MSdJ+T8007O2aectOxYG9pueY93W7m76d/u7n+W5BPd/YYkX5PkixYbCwAAWM9Ne+/N1bftzX37HkiS3LfvgVx9297ctPfeJSdb33bLy9ZK4memfz9WVU9LckqSxy0uEgAAsJE9d9yT+/vBY/f3bHwVbbe8bO1008uq6tQkP5JkV5LPSvLPFpoKAABY1/4jclsdX7btlpctlMTu/oXp4buSPClJqursRYYCAADWd+LOE9YtWCfu3MpJgkffdsvLQU43raovrqqXVNXjpumnV9WbkrznqKQDAAAe5PzTTs6OevDYjpqNr6LtlpdNSmJV/X9JXp/kxUn+W1X9eJL/keT3k5x7dOIBAADzzj7lpFzw+FPGkbgTd56QCx5/ysreLXS75WXz002/JskF3f3J6ZrEm5M8rbs/elSSAQAA6zr7lJO2VcnabnmPd5udbvrJ7v5kknT33Uk+rCACAAAc2zY7kvikqto1N33O/HR3v2hxsQAAAFiGzUriRQdM//QigwAAALB8G5bE7v6doxkEAACA5fPhJAAAAAxKIgAAAIOSCAAAwLDZjWuSJFX160n6gOG9SXYnee3+j8kAAABg+9vKkcQbk3w8yeumr79Kck+Sp0zTAAAAHCMOeiQxyZd09xfOTf96VV3Z3V9YVXsWFQwAAICjbytHEj+rqs7ePzE9/qxp8tObrVhVF1bVdVV1fVW9aoNlvqGqrq2qPVX1prnx+6vqmulr1xZyAgAAcJi2ciTxB5K8u6puSFJJzkny3VX1yCRv2GilqtqR5NIkz0tyS5Irq2pXd187t8y5SX44ybO7++6qetzcU9zX3c845O8IAACAh+ygJbG7f2Mqc+dNQ9fN3azmNZus+qwk13f3jUlSVZcnuSjJtXPLfHuSS7v77um1/vIQ8wMAAHAEbfUjMJ6Z5PwkfyfJN1TVN29hnTOS3Dw3fcs0Nu8pSZ5SVe+pqvdV1YVz8x5RVbun8a9d7wWq6hXTMrtvv/32LX4rAAAAbGQrH4HxxiRPTnJNkvun4U7yS0fo9c9N8pwkZyZ5V1V9fnd/LMkTuvvWqnpSkt+qqj/s7hvmV+7uy5JcliRra2sHfkwHAAAAh2gr1ySuJXlqdx9qCbs1yVlz02dOY/NuSfL73f2ZJB+pqg9lVhqv7O5bk6S7b6yqK5JckOSGAAAAsDBbOd30j5J89kN47iuTnFtV51TVw5JcnOTAu5S+NbOjiKmq0zI7/fTGqjq1qh4+N/7sPPhaRgAAABZgK0cST0tybVW9P8mn9g9294s2W6m791XVK5O8PcmOJK/v7j1VdUmS3d29a5r3/Kq6NrNTWX+wu++sqi9J8tqqeiCzIvuT83dFBQAAYDHqYGeRVtWXrzfe3b+zkEQP0draWu/evXvZMQAAAJaiqq7q7rXDfZ6tfATGSpVBAAAAFmfDklhV7+7uL62qezK7m+mYlaS7+1ELTwcAAMBRtWFJ7O4vnf49+ejFAQAAYJm2cuOaVNWOJI+fX767b1pUKAAAAJbjoCWxqr43yY8muS3JA9NwJ3n6AnMBAACwBFs5kvi/J/nc7r5z0WEAAABYrhO2sMzNSfYuOggAAADLt5UjiTcmuaKq/luST+0f7O5/tbBUAAAALMVWSuJN09fDpi8AAACOUZuWxOmupk/p7m86SnkAAABYok2vSezu+5M8oaocQQQAADgObPWaxPdU1a4kn9g/6JpEAACAY89WSuIN09cJSU5ebBwAAACW6aAlsbv/+dEIAgAAwPIdtCRW1elJfijJ+UkesX+8u79ygbkAAABYgk1vXDP55SR/kuScJP88yUeTXLnATAAAACzJVkriY7v73yf5THf/Tnd/SxJHEQEAAI5BW7lxzWemf/+8qr4myZ8lecziIgEAALAsWymJP15VpyT5gST/Jsmjknz/QlMBAACwFFu5u+l/nR7uTfIVi40DAADAMh30msSqekpV/WZV/dE0/fSq+pHFRwMAAOBo28qNa16X5IczXZvY3R9McvEiQwEAALAcWymJJ3X3+w8Y27eIMAAAACzXVkriHVX15CSdJFX1kiR/vtBUAAAALMVW7m76PUkuS3JeVd2a5CNJvmmhqQAAAFiKgx5J7O4bu/u5SU5Pcl53f2mSr1t4MgAAAI66rZxumiTp7k909z3T5D9ZUB4AAACWaMsl8QB1RFMAAACwEh5qSewjmgIAAICVsOGNa6rqnqxfBivJiQtLBAAAwNJsWBK7++SjGQQAAIDle6inmwIAAHAMUhIBAAAYlEQAAAAGJREAAIBBSQQAAGBQEgEAABiURAAAAAYlEQAAgEFJBAAAYFASAQAAGJREAAAABiURAACAQUkEAABgUBIBAAAYlEQAAAAGJREAAIBBSQQAAGBQEgEAABiURAAAAAYlEQAAgEFJBAAAYFASAQAAGJREAAAABiURAACAQUkEAABgUBIBAAAYlEQAAAAGJREAAIBBSQQAAGBQEgEAABiURAAAAAYlEQAAgEFJBAAAYFASAQAAGJREAAAABiURAACAQUkEAABgUBIBAAAYlEQAAAAGJREAAIBBSQQAAGBQEgEAABiURAAAAAYlEQAAgEFJBAAAYFASAQAAGJREAAAABiURAACAQUkEAABgUBIBAAAYlEQAAACGhZbEqrqwqq6rquur6lUbLPMNVXVtVe2pqjfNjb+sqj48fb1skTkBAACY2bmoJ66qHUkuTfK8JLckubKqdnX3tXPLnJvkh5M8u7vvrqrHTeOPSfKjSdaSdJKrpnXvXlReAAAAFnsk8VlJru/uG7v700kuT3LRAct8e5JL95e/7v7LafwFSd7R3XdN896R5MIFZgUAACCLLYlnJLl5bvqWaWzeU5I8pareU1Xvq6oLD2HdVNUrqmp3Ve2+/fbbj2B0AACA49Oyb1yzM8m5SZ6T5KVJXldVj97qyt19WXevdffa6aefvqCIAAAAx49FlsRbk5w1N33mNDbvliS7uvsz3f2RJB/KrDRuZV0AAACOsEWWxCuTnFtV51TVw5JcnGTXAcu8NbOjiKmq0zI7/fTGJG9P8vyqOrWqTk3y/GkMAACABVrY3U27e19VvTKzcrcjyeu7e09VXZJkd3fvyl+XwWuT3J/kB7v7ziSpqh/LrGgmySXdfdeisgIAADBT3b3sDEfE2tpa7969e9kxAAAAlqKqrurutcN9nmXfuAYAAIAVoiQCAAAwKIkAAAAMSiIAAACDkggAAMCgJAIAADAoiQAAAAxKIgAAAIOSCAAAwKAkAgAAMCiJAAAADEoiAAAAg5IIAADAoCQCAAAwKIkAAAAMSiIAAACDkggAAMCgJAIAADAoiQAAAAxKIgAAAIOSCAAAwKAkAgAAMCiJAAAADEoiAAAAg5IIAADAoCQCAAAwKIkAAAAMSiIAAACDkggAAMCgJAIAADAoiQAAAAxKIgAAAIOSCAAAwKAkAgAAMCiJAAAADEoiAAAAg5IIAADAoCQCAAAwKIkAAAAMSiIAAACDkggAAMCgJAIAADAoiQAAAAxKIgAAAIOSCAAAwKAkAgAAMCiJAAAADEoiAAAAg5IIAADAoCQCAAAwKIkAAAAMSiIAAACDkggAAMCgJAIAADAoiQAAAAxKIgAAAIOSCAAAwKAkAgAAMCiJAAAADEoiAAAAg5IIAADAoCQCAAAwKIkAAAAMSiIAAACDkggAAMCgJAIAADAoiQAAAAxKIgAAAIOSCAAAwKAkAgAAMCiJAAAADEoiAAAAg5IIAADAoCQCAAAwKIkAAAAMSiIAAACDkggAAMCgJAIAADAoiQAAAAxKIgAAAIOSCAAAwLDQklhVF1bVdVV1fVW9ap35L6+q26vqmunr2+bm3T83vmuROQEAAJjZuagnrqodSS5N8rwktyS5sqp2dfe1Byz6K939ynWe4r7ufsai8gEAAPA3LfJI4rOSXN/dN3b3p5NcnuSiBb4eAAAAh2mRJfGMJDfPTd8yjR3oxVX1wap6S1WdNTf+iKraXVXvq6qvXe8FquoV0zK7b7/99iMYHQAA4Pi07BvX/HqSJ3b305O8I8kb5uY9obvXknxjktdU1ZMPXLm7L+vute5eO/30049OYgAAgGPYIkvirUnmjwyeOY0N3X1nd39qmvyFJM+cm3fr9O+NSa5IcsECswIAAJDFlsQrk5xbVedU1cOSXJzkQXcprarPmZt8UZI/nsZPraqHT49PS/LsJAfe8AYAAIAjbGF3N+3ufVSqstMAABFASURBVFX1yiRvT7Ijyeu7e09VXZJkd3fvSvJ9VfWiJPuS3JXk5dPqn5fktVX1QGZF9ifXuSsqAAAAR1h197IzHBFra2u9e/fuZccAAABYiqq6arqvy2FZ9o1rAAAAWCFKIgAAAIOSCAAAwKAkAgAAMCiJAAAADEoiAAAAg5IIAADAoCQCAAAwKIkAAAAMSiIAAACDkggAAMCgJAIAADAoiQAAAAxKIgAAAIOSCAAAwKAkAgAAMCiJAAAADEoiAAAAg5IIAADAoCQCAAAwKIkAAAAMSiIAAACDkggAAMCgJAIAADAoiQAAAAxKIgAAAIOSCAAAwKAkAgAAMCiJAAAADEoiAAAAg5IIAADAoCQCAAAwKIkAAAAMSiIAAACDkggAAMCgJAIAADAoiQAAAAxKIgAAAIOSCAAAwKAkAgAAMCiJAAAADEoiAAAAg5IIAADAoCQCAAAwKIkAAAAMSiIAAACDkggAAMCgJAIAADAoiQAAAAxKIgAAAIOSCAAAwKAkAgAAMCiJAAAADEoiAAAAg5IIAADAoCQCAAAwKIkAAAAMSiIAAACDkggAAMCgJAIAADAoiQAAAAxKIgAAAIOSCAAAwKAkAgAAMCiJAAAADEoiAAAAg5IIAADAoCQCAAAwKIkAAAAMSiIAAACDkggAAMCgJAIAADAoiQAAAAxKIgAAAIOSCAAAwKAkAgAAMCiJAAAADEoiAAAAg5IIAADAoCQCAAAwKIkAAAAMSiIAAADDQktiVV1YVddV1fVV9ap15r+8qm6vqmumr2+bm/eyqvrw9PWyReYEAABgZueinriqdiS5NMnzktyS5Mqq2tXd1x6w6K909ysPWPcxSX40yVqSTnLVtO7di8oLAADAYo8kPivJ9d19Y3d/OsnlSS7a4rovSPKO7r5rKobvSHLhgnICAAAwWdiRxCRnJLl5bvqWJF+0znIvrqovS/KhJN/f3TdvsO4ZB65YVa9I8opp8lNV9UdHIvhRdFqSO5Yd4hDIu1jyLpa8iyXvYsm7WPIulryLJe9ibbe8n3sknmSRJXErfj3Jm7v7U1X1HUnekOQrt7pyd1+W5LIkqard3b22mJiLsd0yy7tY8i6WvIsl72LJu1jyLpa8iyXvYm3HvEfieRZ5uumtSc6amz5zGhu6+87u/tQ0+QtJnrnVdQEAADjyFlkSr0xyblWdU1UPS3Jxkl3zC1TV58xNvijJH0+P357k+VV1alWdmuT50xgAAAALtLDTTbt7X1W9MrNytyPJ67t7T1VdkmR3d+9K8n1V9aIk+5LcleTl07p3VdWPZVY0k+SS7r7rIC952SK+jwXbbpnlXSx5F0vexZJ3seRdLHkXS97Fknexjsu81d1H4nkAAAA4BizydFMAAAC2GSURAACAYVuUxKq6sKquq6rrq+pV68x/eFX9yjT/96vqidP4E6vqvqq6Zvr6+RXJ+2VV9QdVta+qXnLAvJdV1Yenr5dtg7z3z23fXQeuu6S8/6Sqrq2qD1bVb1bVE+bmreL23SzvKm7f76yqP5wyvbuqnjo374en9a6rqhesct5V3T/MLffiquqqWpsbW7ntu1HeVd2+VfXyqrp9Lte3zc1bxf3DZnmP+v5hK5mnZb5h2q/tqao3zY2v3DY+SN5V3Af/zFymD1XVx+bmrdz2PUjeVdy+Z1fVb1fV1TX7//JXz81buX3wRnlXeB/8hJr9rvPBqrqiqs6cm7eK79/N8i7j/fv6qvrL2uCz4WvmZ6fv54NV9QVz8w5t+3b3Sn9ldtObG5I8KcnDknwgyVMPWOa7k/z89PjiJL8yPX5ikj9awbxPTPL0JL+U5CVz449JcuP076nT41NXNe807+MruH2/IslJ0+Pvmns/rOr2XTfvCm/fR809flGS/z49fuq0/MOTnDM9z44VzruS+4dpuZOTvCvJ+5KsrfL23STvSm7fzG6Q9m/XWXdV9w/r5p3mHdX9wyFkPjfJ1fu3X5LHrfg2XjfvMrbxVn/m5pb/3sxuDLiy23ejvKu6fTO76cd3TY+fmuSjc49Xbh+8Sd4nZjX3wf8pycumx1+Z5I2r/P7dKO8y3r/Ta35Zki/Y6L9tkq9O8rYkleTvJvn9h7p9t8ORxGclub67b+zuTye5PMlFByxzUZI3TI/fkuSrqqqOYsZ5B83b3R/t7g8meeCAdV+Q5B3dfVd3353kHUkuXOG8y7CVvL/d3fdOk+/L7HM2k9XdvhvlXYat5P2ruclHJtl/96uLklze3Z/q7o8kuX56vlXNuwxb2Z8lyY8l+X+TfHJubCW37yZ5l2GredezkvuHFbSVzN+e5NJpO6a7/3IaX9VtvFHeZTjU98RLk7x5eryq23fefN5l2EreTvKo6fEpSf5seryq++CN8i7DVvI+NclvTY9/e27+qr5/N8q7FN39rsw+EWIjFyX5pZ55X5JH1+wjBw95+26HknhGkpvnpm+ZxtZdprv3Jdmb5LHTvHOmQ/C/U1V/b9Fhs7W8i1j3oTrc13xEVe2uqvdV1dce2WjrOtS835rZX1QeyrpHwuHkTVZ0+1bV91TVDUl+Ksn3Hcq6R9jh5E1WcP8wnRpyVnf/t0NddwEOJ2+ygtt38uLpNJy3VNVZh7jukXQ4eZOjv39Itpb5KUmeUlXvmbJdeAjrHmmHkzdZ0X1wMjsNLrMjWvt/gV3V7Ztk3bzJam7fVyf5h1V1S5LfyOzo51bXPdIOJ2+ymvvgDyT5+unx1yU5uaoeu8V1j7TDyZssZx98MBt9T4e8fRf2OYkr4s+TnN3dd1bVM5O8tarOP+DIAofnCd19a1U9KclvVdUfdvcNyw6VJFX1D5OsJfnyZWfZig3yruT27e5Lk1xaVd+Y5EeSHJVrBx6qDfKu3P6hqk5I8q8yfWbsqjtI3pXbvpNfT/Lm7v5UVX1HZmehfOWSM21ms7wruX/I7HeLc5M8J7MzI95VVZ+/1ESbWzdvd38sq7uNk9nlNW/p7vuXHWSL1su7itv3pUl+sbt/uqq+OMkbq+ppS860mY3yruo++P9M8m+r6uWZXaZwa5JVfg9vlncV379HzHY4knhrkvm/nJ45ja27TFXtzOxw+53TKQF3Jkl3X5XZecdPWYG8i1j3oTqs1+zuW6d/b0xyRZILjmS4dWwpb1U9N8k/TfKi7v7Uoax7hB1O3pXdvnMuT7L/r2cru33njLwrun84OcnTklxRVR/N7HqCXTW7Gcwqbt8N867o9k133zn3M/YLSZ651XUX4HDyLmP/kGxtO92SZFd3f2Y6Le9DmZWwldzG2Tjvqu+DL86DT91c1e2734F5V3X7fmuSX51yvTfJI5KctsV1j7SHnHeF98F/1t1f390XZPZ7T6Y/yKzk9t0k77L2wQez0fd06Nu3j/IFl4f6ldlf+G7M7BSF/ReVnn/AMt+TB9+45lenx6dnuqg4s4tSb03ymGXnnVv2F/M3b1zzkcwuKD11erzKeU9N8vDp8WlJPpxNLlg/iu+HCzLbGZ57wPhKbt9N8q7q9j137vELk+yeHp+fB1/Uf2MWf1H/4eRd6f3DtPwV+esbwazk9t0k70pu3ySfM/f465K8b3q8qvuHjfIe9f3DIWS+MMkb5rLdnNklIKu6jTfKu5L74Gm585J8NEnNja3k9t0k70pu38wu+Xj59PjzMrvGr7Ki++BN8q7qPvi0JCdMj38iySWr/P7dJO9S9sHT6z0xG9+45mvy4BvXvP+hbt+FfyNHaGN8dWZ/2bshyT+dxi7J7KhLMvuryX/K7CLi9yd50jT+4iR7klyT5A+SvHBF8n5hZn+5/ESSO5PsmVv3W6bv4/ok/3iV8yb5kiR/OP1Q/WGSb12RvO9Mctv03/2azP5CvMrbd928K7x9//Xcz9VvZ26Hmtlf2W5Icl2Sv7/KeVd1/3DAsldkKl2run03yruq2zfJ/zPl+sD0fjhvbt1V3D+sm3dZ+4ctZq7MTkO+dsp28Ypv43XzLmsbb+VnLrPr0H5ynXVXbvtulHdVt29mNyp5z5TrmiTPn1t35fbBG+XN6u6DX5JZofpQZmdHPHyV378b5V3i+/fNmZ1K/JnMfjf/1iTfmeQ7p/mV5NLp+/nDPPh3iEPavjWtBAAAANvimkQAAACOEiURAACAQUkEAABgUBIBAAAYlEQAAAAGJRGAhauqx1bVNdPXX1TVrXPTDzvIumtV9bNbeI3fO0JZn1NVe6vq6qq6rqreVVX/YIvrfclDfM3XTNvE/5cBWLqdyw4AwLGvu+9M8owkqapXJ/l4d//L/fOramd379tg3d1Jdm/hNR5SQdvA73b3P5iyPSPJW6vqvu7+zU3WeU6Sjyc5pLI6FcOvy+xD3L88s89DPOI228YAMM9fLAFYiqr6xar6+ar6/SQ/VVXPqqr3Tkfwfq+qPnda7jlV9V+nx6+uqtdX1RVVdWNVfd/c8318bvkrquotVfUnVfXLVVXTvK+exq6qqp/d/7yb6e5rMvtw5VdOz/HCqvr9Kec7q+rxVfXEzD7Q+Puno6N/b73lNniJ52T2odc/l+Slc9/P46vqv1TVB6avL5nGv7mqPjiNvXFuW75kg23xu1W1K7MPi09VvXX6/vdU1Svm1rmwqv5get7frKoTqurDVXX6NP+Eqrp+/zQAxy5HEgFYpjOTfEl3319Vj0ry97p7X1U9N8m/SPLiddY5L8lXJDk5yXVV9XPd/ZkDlrkgyflJ/izJe5I8u6p2J3ltki/r7o9U1ZsPIecfJPnB6fG7k/zd7u6q+rYkP9TdP1BVP5+5I6RVdeqByyX5gXWe+6VJ3pzk15L8i6r6W9P387NJfqe7v66qdiT5rKo6P8mPTNvsjqp6zBayf0GSp3X3R6bpb+nuu6rqxCRXVtV/zuyPxq+b2zaP6e4Hquo/JvmmJK9J8twkH+j+n+3dTYjVVRjH8e8PK0SEESIiUBgDIaIXpIiEtBLCCAxcRLSxiDJqYdCqFkEgUbjQoFxEoQRBiOTCktxU1CoEsRCRCErQFpUUYhYDytPif+byV5zxjuDcmfx+VofDc8595myG556XW3/MYN0kSfOQRaIkaZT2VNX51h4DPkqyAijg+inG7K+qCWAiye/AzcDJi2IOVtVJgCTfA+N0R0F/7hVLnwCbGE567aXA7iS3ADcAv1x6yOXj2n3Mx4BXqupM21VdB3wOrAU2ArQ1Op1kI92anWr9fw6R+8He3wywOcmG1l4GrABuAr6djOvNu5OueH0HeBbYNcTnSZLmOY+bSpJG6WyvvQX4uqruANYDC6cYM9Frn+fSX3gOEzMTK4Fjrf0u8F5V3Qm8ME2ew8StA5YAR5IcBx6gd+R0Bs7R/qe3O479x4AGa5zkIbodwVVVdTdweJr8qaoTwG9J1gL3AV9cQW6SpHnGIlGSNFeMAb+29jNXYf4fgVvb/UGAJ4cZlOQu4HVgR+vq5/l0L/QM3RFYLhPX9xTwXFWNV9U4sBx4JMki4EvgxZbDgiRjwFfAE0lubP2Tx02PA/e09uNMvQs7BvxVVf8kuQ24v/V/B6xJsvyieQE+BD7mwl1fSdL/mEWiJGmu2Aq8leQwV+E6RFX9C7wEHEhyiK6oOz1F+OrJn8CgKw439142fQPY0+Y41RvzGbBh8uGaaeIAaIXgo8D+Xo5n6e48rgdeBh5OcgQ4BNxeVUeBN4FvkvwAbGtDPwAebH2ruHCHtu8AcF2SY8DbdMUh7Z7hJmBvm2N3b8w+YDEeNZWka0aqatQ5SJI0K5Isrqq/22unO4Cfqmr7qPOay5LcC2yvqtWjzkWSNDvcSZQkXUuebw/ZHKU7evn+iPOZ05K8CnwKvDbqXCRJs8edREmSJEnSgDuJkiRJkqQBi0RJkiRJ0oBFoiRJkiRpwCJRkiRJkjRgkShJkiRJGvgPq8X5e8jwnjkAAAAASUVORK5CYII=\n","text/plain":["<Figure size 1080x720 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 1080x720 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"onnqJYTEq0l3"},"source":["#ID3 Decision Tree\n","\n","In this section, you will implement the ID3 Decision Tree algorithm. The training is for a **binary classification** task i.e. each instance will have a class value of 0 or 1. Also, assume that you are given **all binary-valued attributes** and that there are **no missing values** in the train or test data. \n"]},{"cell_type":"markdown","metadata":{"id":"eDNztBkTtRPw"},"source":["## Algorithm\n","\n","(100 points)\n","\n","Following are the data files that will be provided to you for the ID3 algorithm implementation.\n","\n","*   Training file - 'id3-train.dat'\n","*   Testing file - 'id3-test.dat'\n","\n","*Both these files should be present in the same folder as this code file.* In these files, only non-space characters are relevant. The first line contains the attribute names. All the other lines are example instances to be used for the algorithm. Each column holds values of the attributes, whereas the last column holds the class label for that instance.\n","\n","In a decision tree, if you reach a leaf node but still have examples that belong to different classes, then choose the most frequent class (among the instances at the leaf node). If you reach a leaf node in the decision tree and have no examples left or the examples are equally split among multiple classes, then choose the class that is most frequent in the entire training set. You do not need to implement pruning. Also, don’t forget to use logarithm base 2 when computing entropy and set (0 log 0) to 0.\n","\n","Write the code in the following code block, structure is provided. Instructions on the steps to follow are provided as comments. The code should output the following 3 things:\n","\n","*   Print the Decision Tree created, in the following example format:\n","\n","    ```\n","    attr1 = 0 :\n","        attr2 = 0 :\n","            attr3 = 0 : 1\n","            attr3 = 1 : 0\n","        attr3 = 1 :\n","            attr4 = 0 : 0\n","            attr4 = 1 : 1\n","    attr1 = 1 :\n","        attr2 = 1 : 1\n","\n","    ```\n","\n","*   Accuracy on the Training data = x %\n","*   Accuracy on the Test data = x %\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"wTLz1lGYQJKS"},"source":["# Data file name variables\n","train = basePath + \"id3-train.dat\"\n","test = basePath + \"id3-test.dat\"\n","\n","class Node(object):\n","    def __init__(self, parent):\n","        self.parent = parent\n","        self.children = []\n","        self.splitAttribute = None\n","        self.splitAttributeValue = None\n","        self.count = None\n","        self.cls = None\n","        self.prediction = None\n","        self.isLeafNode = False\n","        # self.entropy = entropy"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HwHYC6xjQP5D"},"source":["# Pseudocode for the ID3 algorithm. Use this to create function(s).\n","# def ID3(data, root, attributesRemaining):\n","def ID3(orig_data, curr_data, root, attributesRemaining, classif):\n","    # get the total entropy for this dataset, used to get IG\n","    total_entropy = GetEntropyForAllData(curr_data, classif)\n","    # print('total_entropy= ' + str(total_entropy) + ' for dataset: \\n' + str(curr_data))#-debug\n","    root.entropy = total_entropy\n","    # check1, check1OccurrenceCount = np.unique(curr_data[classif], return_counts=True)\n","    origVals, origOccur = np.unique(orig_data[classif], return_counts=True)\n","    areEquallySplit = True\n","    # if origVals[0] != origVals[1]:\n","    if len(origVals) != 1:\n","        areEquallySplit = False\n","    # If you reach a leaf node in the decision tree and have no examples left or the examples are equally split among multiple classes\n","    if attributesRemaining == [] or areEquallySplit: # or something***************\n","    #     # Choose and the class that is most frequent in the entire training set and return the updated tree\n","        i = 0\n","        prev = 0\n","        saveIdx = 0\n","        while i < len(origVals) - 1:\n","            if origOccur[i] > prev:\n","                prev = origOccur[i]\n","                saveIdx = i\n","            i += 1\n","\n","        # root.splitAttributeValue = origVals[saveIdx]\n","        root.prediction = origVals[saveIdx]\n","        root.isLeafNode = True\n","        return root\n","\n","    # # If all the instances have only one class label\n","    check2 = np.unique(curr_data[classif])\n","    if len(check2) == 1:\n","        # Make this as the leaf node and use the label as the class value of the node and return the updated tree\n","        root.splitAttribute = check2[0]\n","        root.isLeafNode = True\n","        root.prediction = check2[0]\n","        # print('all rows in this dataset are the same classification')\n","    #     return root\n","        return root\n","    check3, check3OccurrenceCount = np.unique(curr_data[classif], return_counts=True)\n","    # # If you reached a leaf node but still have examples that belong to different classes (there are no remaining attributes to be split)\n","    if len(check3) > 1 and attributesRemaining == []:\n","    #     # Assign the most frequent class among the instances at the leaf node and return the updated tree\n","        i = 0\n","        idx = 0\n","        mostFreq = 0\n","        mostFreqClass = ''\n","        while i < len(check3) - 1:\n","            if check3OccurrenceCount[i] > mostFreq:\n","                idx = i\n","        # root.splitAttributeValue = check3[idx]\n","        root.prediction = check3[idx]\n","        root.isLeafNode = True\n","    # print('case 3, examples that belong to different classes and no remaining attributes')\n","        return root\n","\n","\n","    # Find the best attribute to split by calculating the maximum information gain from the attributes remaining by calculating the entropy\n","    infoGain = {}\n","    # Calculate info gain and save into dictionary with key of attribute and value of IG\n","    for attr in attributesRemaining:\n","        infoGain[attr] = total_entropy - GetEntropyAnyDataType(curr_data, attr, classif)\n","    # print('IG:' + str(infoGain)) #-debug\n","\n","    # Select the best attribute from infoGain\n","    # get best attribute source: https://stackoverflow.com/questions/268272/getting-key-with-maximum-value-in-dictionary\n","    bestAttr = max(infoGain.items(), key=operator.itemgetter(1))[0]\n","\n","    # Split the tree using the best attribute and recursively call the ID3 function using DFS to fill the sub-tree\n","    root.splitAttribute = bestAttr\n","    attributesRemaining.remove(bestAttr)\n","    copyOfAttributesRemaining = attributesRemaining.copy()\n","    uniqueAttrs = np.unique(orig_data[bestAttr])\n","    # print('unique attributes for best attribute: ' + bestAttr + '= ' + str(uniqueAttrs))  # -debug\n","    for v in uniqueAttrs:\n","        # print('v: ' + v + ' main attr: ' + bestAttr)#-debug\n","        # create a child node with\n","        child = Node(root)\n","        child.splitAttributeValue = v\n","        # split_data = data where value for bestAttr is v\n","        split_data = curr_data[curr_data[bestAttr] == v]\n","        child.count = len(split_data)\n","        root.children.append(child)\n","        # print('splitting on attribute ' + str(bestAttr) + ' for value v= ' + str(v))  # -debug\n","        ID3(orig_data, split_data, child, copyOfAttributesRemaining, classif)\n","    # return the root as the tree\n","    return root"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nYNLSxcM4Y2V"},"source":["# def GetEntropy(data, attr):\n","#   posVal = 0\n","#   negVal = 0\n","#   instanceCount = len(data)\n","#   for index, attrs in data.iterrows():\n","#     if attrs['class'] == 1 and attrs[attr] == 1:\n","#       posVal += 1\n","#     if attrs['class'] == 0 and attrs[attr] == 1:\n","#       negVal += 1\n","#   pVal = posVal/instanceCount\n","#   nVal = negVal/instanceCount\n","#   posExpr = 0\n","#   negExpr = 0\n","#   if pVal != 0:\n","#     posExpr = -(pVal) * math.log2(pVal)\n","#   if nVal != 0:\n","#     negExpr = -(nVal) * math.log2(nVal)\n","#   entropy = posExpr + negExpr\n","#   print(entropy)\n","#   return entropy\n","  \n","def GetEntropyForAllData(data, classif):\n","    posVal = 0\n","    negVal = 0\n","    instanceCount = len(data)\n","    for index, attrs in data.iterrows():\n","        if attrs[classif] == 1 or attrs[classif] == 'yes':\n","        # if attrs['class'] == 'yes':#-debug for weather\n","            posVal += 1\n","        if attrs[classif] == 0 or attrs[classif] == 'no':\n","        # if attrs['play'] == 'no':#-debug for weather\n","            negVal += 1\n","    pVal = posVal / instanceCount\n","    nVal = negVal / instanceCount\n","    posExpr = 0\n","    negExpr = 0\n","    if pVal != 0:\n","        posExpr = -(pVal) * math.log2(pVal)\n","    if nVal != 0:\n","        negExpr = -(nVal) * math.log2(nVal)\n","    entropy = posExpr + negExpr\n","    # print(entropy) #-debug\n","    return entropy\n","\n","def GetEntropyAnyDataType(data, attr, classif):\n","    # get the total size of this data set\n","    instanceCount = len(data)\n","    possibleFeatures = np.unique(data[attr])\n","    # used to count positive occurrences\n","    dictClassCount = {}\n","    # used to count the full number of occurrences of the specific attributes\n","    dictClassCountLen = {}\n","    # this loop populates the dictionaries, dictClassCount initializes the keys with values of 0\n","    # dictClassCountLen gets the total number of rows with that feature for the specific attribute\n","    for possibleFeature in possibleFeatures:\n","        dictClassCount[possibleFeature] = 0\n","        dictClassCountLen[possibleFeature] = len(data[data[attr] == possibleFeature])\n","\n","    # this loop gets the positive count for the feature for that specific attribute\n","    # Ex: when outlook = sunny and play = yes -> increment dictClassCount[sunny]\n","    for index, attrs in data.iterrows():\n","        for possibleFeature in possibleFeatures:\n","            # need a way to determine what a 'good' classification is.... parameter?\n","            if attrs[attr] == possibleFeature and (attrs[classif] == 'yes' or attrs[classif] == 1):\n","                # increment the count of good classifications for the specific attribute\n","                dictClassCount[attrs[attr]] += 1\n","    # calculate conditional entropy (weighted average)\n","    conditionalEntropy = 0\n","    for attrKey, attrCount in dictClassCount.items():\n","        # positive case\n","        posExpr = 0\n","        # number of positive values for specific attribute  / total occurrences of that specific attribute\n","        pVal = dictClassCount[attrKey] / dictClassCountLen[attrKey]\n","        # negative case\n","        negExpr = 0\n","        # number of negative values for specific attribute  / total occurrences of that specific attribute\n","        nVal = (dictClassCountLen[attrKey] - dictClassCount[attrKey]) / dictClassCountLen[attrKey]\n","        # get entropy for the positive occurrences\n","        if pVal != 0:\n","            posExpr = -(pVal) * math.log2(pVal)\n","        # get entropy for the negative occurrences\n","        if nVal != 0:\n","            negExpr = -(nVal) * math.log2(nVal)\n","        # get sum of entropy\n","        temp = posExpr + negExpr\n","        # instance of 'attr type' / total * temp\n","        numberOfRowsWhereAttrValIsAttrKey = len(data[data[attr] == attrKey])\n","        # update the conditional entropy using the weight of that attribute\n","        conditionalEntropy += numberOfRowsWhereAttrValIsAttrKey / instanceCount * temp\n","    # print('returning conditional entropy for ' + str(attr) + ' = ' + str(conditionalEntropy))#-debug\n","    return conditionalEntropy\n","\n","\n","def PrintTreeWeather(tree, numTabs):\n","    whiteSpace = '\\t' * numTabs\n","    for node in tree.children:\n","        if node.splitAttribute == 'yes' or node.splitAttribute == 'no' or node.splitAttribute == 1 or node.splitAttribute == 0:\n","            print(whiteSpace + str(node.parent.splitAttribute) + ' = ' + str(node.splitAttributeValue) + ' : ' + str(node.splitAttribute) + '\\t--\\t'\n","                  + str(node.count))\n","        else:\n","            numTabs += 1\n","            whiteSpace = '\\t' * numTabs\n","            print(str(node.parent.splitAttribute) + ' = ' + str(node.splitAttributeValue) + ' :')\n","            for child in node.children:\n","                print(whiteSpace + str(child.parent.splitAttribute) + ' = ' + str(child.splitAttributeValue) + ' : ' + str(child.splitAttribute) + '\\t--\\t'\n","                  + str(child.count))\n","                \n","def PrintTreeTest(node, numTabs):\n","    whiteSpace = '\\t' * numTabs\n","    if node.children == []:\n","        print(whiteSpace + str(node.parent.splitAttribute) + ' = ' + str(node.splitAttributeValue) + ' : ' + str(node.splitAttribute))\n","    numTabs += 1\n","    whiteSpace = '\\t' * numTabs\n","    for child in node.children:\n","        PrintTreeTest(child, numTabs)\n","\n","\n","def printTree(node, depth):\n","    whiteSpace = '\\t' * depth\n","    if depth != 0:\n","        # if node.prediction != None:\n","        if node.isLeafNode:\n","            print(whiteSpace + str(node.parent.splitAttribute) + ' = ' + str(node.splitAttributeValue) + ' : '\n","                  + str(node.prediction) + '\\t--\\t' + str(node.count))\n","        else:\n","            print(whiteSpace + str(node.parent.splitAttribute) + ' = ' + str(node.splitAttributeValue) + ' : ')# + str(node.prediction))\n","    for child in node.children:\n","        doesThisWork = printTree(child, depth+1)\n","    return whiteSpace\n","\n","def Prediction(row, node):\n","    # base case, reached a leaf node\n","    if node.isLeafNode:\n","        return node.prediction\n","    for child in node.children:\n","        # if the split attribute in this row is the same as the child's split value\n","        if row[node.splitAttribute] == child.splitAttributeValue:\n","            # continue down the tree\n","            retVal = Prediction(row, child)\n","    return retVal"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XELGzRDftS77","colab":{"base_uri":"https://localhost:8080/"},"outputId":"72a8bd14-1895-414f-b5ac-262b5275cdbe"},"source":["# Following is the base code structure. Feel free to change the code structure as you see fit, maybe even create more functions.\n","df_train = pd.read_table(train)\n","df_test = pd.read_table(test)\n","# df_weather = pd.read_csv('weather.csv')\n","# Read the first line in the training data file, to get the number of attributes\n","numAttr = len(df_train.columns) - 1\n","attrList = list(df_train.columns)\n","# save the title of the classification column\n","classif = attrList[-1]\n","# remove the classification from the attributes list\n","attrList.remove('class')\n","# attrListWeather = list(df_weather.columns)\n","# save the title of the classification column\n","# classifWeather = attrListWeather[-1]\n","# remove the classification from the attributes list\n","# attrListWeather.remove('play')\n","\n","# get the initial entropy for the training data\n","total_entropy = GetEntropyForAllData(df_train, classif)\n","# get the inital entropy for the weather data\n","# total_entropy_weather = GetEntropyForAllData(df_weather, classifWeather)\n","# print('total entropy weather: ' + str(total_entropy_weather))#-debug\n","# create the root node\n","root = Node(None)\n","# training data\n","print('ASSIGNMENT DATA START ****************************************')\n","tree = ID3(df_train, df_train, root, attrList, classif)\n","\n","### TODO\n","# Print the tree in the example format mentioned.\n","printTree(tree, 0)\n","# Use the above created tree to predict the training data and print the accuracy as \"Accuracy on the Training data = x %\"\n","# For each training instance, predict the output label\n","# Compare it with the ground truth class label and calculate the accuracy accordingly\n","countCorrect = 0\n","totalRows = len(df_train.index)\n","for index, row in df_train.iterrows():\n","    predict = Prediction(row, tree)\n","    if predict == row['class']:\n","        countCorrect += 1\n","# Use the above created tree to predict the testing data and print the accuracy as \"Accuracy on the Test data = x %\"\n","print('\\n\\nAccuracy for training data: ' + str(countCorrect / totalRows * 100) + '%')\n","# For each testing instance, predict the output label\n","countCorrect = 0\n","totalRows = len(df_test.index)\n","for index, row in df_test.iterrows():\n","    predict = Prediction(row, tree)\n","    if predict == row['class']:\n","        countCorrect += 1\n","# Compare it with the ground truth class label and calculate the accuracy accordingly\n","print('Accuracy for testing data: ' + str(countCorrect / totalRows * 100) + '%')\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["ASSIGNMENT DATA START ****************************************\n","\tattr5 = 0 : \n","\t\tattr6 = 0 : \n","\t\t\tattr2 = 0 : \n","\t\t\t\tattr1 = 0 : \n","\t\t\t\t\tattr4 = 0 : \n","\t\t\t\t\t\tattr3 = 0 : 0\t--\t9\n","\t\t\t\t\t\tattr3 = 1 : 0\t--\t12\n","\t\t\t\t\tattr4 = 1 : 0\t--\t27\n","\t\t\t\tattr1 = 1 : \n","\t\t\t\t\tattr3 = 0 : 0\t--\t30\n","\t\t\t\t\tattr3 = 1 : 0\t--\t25\n","\t\t\tattr2 = 1 : \n","\t\t\t\tattr4 = 0 : \n","\t\t\t\t\tattr3 = 0 : 0\t--\t25\n","\t\t\t\t\tattr3 = 1 : 0\t--\t29\n","\t\t\t\tattr4 = 1 : 0\t--\t57\n","\t\tattr6 = 1 : \n","\t\t\tattr4 = 0 : \n","\t\t\t\tattr1 = 0 : \n","\t\t\t\t\tattr3 = 0 : 0\t--\t21\n","\t\t\t\t\tattr3 = 1 : 0\t--\t20\n","\t\t\t\tattr1 = 1 : 0\t--\t52\n","\t\t\tattr4 = 1 : \n","\t\t\t\tattr3 = 0 : 0\t--\t51\n","\t\t\t\tattr3 = 1 : 0\t--\t39\n","\tattr5 = 1 : \n","\t\tattr3 = 0 : \n","\t\t\tattr4 = 0 : \n","\t\t\t\tattr1 = 0 : \n","\t\t\t\t\tattr2 = 0 : 0\t--\t32\n","\t\t\t\t\tattr2 = 1 : 0\t--\t25\n","\t\t\t\tattr1 = 1 : 0\t--\t43\n","\t\t\tattr4 = 1 : \n","\t\t\t\tattr2 = 0 : 0\t--\t48\n","\t\t\t\tattr2 = 1 : 0\t--\t57\n","\t\tattr3 = 1 : \n","\t\t\tattr2 = 0 : \n","\t\t\t\tattr1 = 0 : 0\t--\t52\n","\t\t\t\tattr1 = 1 : 0\t--\t55\n","\t\t\tattr2 = 1 : 0\t--\t91\n","\n","\n","Accuracy for training data: 72.25%\n","Accuracy for testing data: 68.96551724137932%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"clfJWxWpGU6s"},"source":["## Note for grader\n","\n","This appears to work for the weather data. See the output from the block below."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LjJe0bIbGeyb","outputId":"011304e9-d082-40d7-c39b-180743e039b9"},"source":["#ID3 for weather\n","def ID3Weather(orig_data, curr_data, root, attributesRemaining, classif):\n","    # print()\n","    # get the total entropy for this dataset, used to get IG\n","    total_entropy = GetEntropyForAllData(curr_data, classif)\n","    # print('total_entropy= ' + str(total_entropy) + ' for dataset: \\n' + str(curr_data))#-debug\n","    root.entropy = total_entropy\n","    check1, check1OccurrenceCount = np.unique(curr_data[classif], return_counts=True)\n","    origVals, origOccur = np.unique(orig_data[classif], return_counts=True)\n","    # If you reach a leaf node in the decision tree and have no examples left or the examples are equally split among multiple classes\n","    if attributesRemaining == [] or origOccur[0] == origOccur[1]: # or something***************\n","    #     # Choose and the class that is most frequent in the entire training set and return the updated tree\n","        i = 0\n","        prev = 0\n","        saveIdx = 0\n","        while i < len(origVals) - 1:\n","            if origOccur[i] > prev:\n","                prev = origOccur[i]\n","                saveIdx = i\n","            i += 1\n","\n","        root.splitAttributeValue = origVals[saveIdx]\n","        # print('case 1, at leaf node and no examples left or examples are equally split')#-debug\n","        return root\n","\n","    # # If all the instances have only one class label\n","    check2 = np.unique(curr_data[classif])\n","    if len(check2) == 1:\n","        # Make this as the leaf node and use the label as the class value of the node and return the updated tree\n","        root.splitAttribute = check2[0]\n","        # print('all rows in this dataset are the same classification')#-debug\n","    #     return root\n","        return root\n","    check3, check3OccurrenceCount = np.unique(curr_data[classif], return_counts=True)\n","    # # If you reached a leaf node but still have examples that belong to different classes (there are no remaining attributes to be split)\n","    if len(check3) > 1 and attributesRemaining == []:\n","    # if attributesRemaining == []:\n","    #     # Assign the most frequent class among the instances at the leaf node and return the updated tree\n","        i = 0\n","        idx = 0\n","        mostFreq = 0\n","        mostFreqClass = ''\n","        while i < len(check3) - 1:\n","            if check3OccurrenceCount[i] > mostFreq:\n","                idx = i\n","        root.splitAttributeValue = check3[idx]\n","        # print('case 3, examples that belong to different classes and no remaining attributes')#-debug\n","        return root\n","\n","\n","    # Find the best attribute to split by calculating the maximum information gain from the attributes remaining by calculating the entropy\n","    infoGain = {}\n","    # Calculate info gain and save into dictionary with key of attribute and value of IG\n","    for attr in attributesRemaining:\n","        infoGain[attr] = total_entropy - GetEntropyAnyDataType(curr_data, attr, classif)\n","    # print('IG:' + str(infoGain)) #-debug\n","\n","    # Select the best attribute from infoGain\n","    # get best attribute source: https://stackoverflow.com/questions/268272/getting-key-with-maximum-value-in-dictionary\n","    bestAttr = max(infoGain.items(), key=operator.itemgetter(1))[0]\n","\n","    # Split the tree using the best attribute and recursively call the ID3 function using DFS to fill the sub-tree\n","    root.splitAttribute = bestAttr\n","    attributesRemaining.remove(bestAttr)\n","    uniqueAttrs = np.unique(orig_data[bestAttr])\n","    # print('unique attributes for best attribute: ' + bestAttr + '= ' + str(uniqueAttrs))  # -debug\n","    for v in uniqueAttrs:\n","        # print('v: ' + v + ' main attr: ' + bestAttr)#-debug\n","        # create a child node with\n","        child = Node(root)\n","        child.splitAttributeValue = v\n","        # split_data = data where value for bestAttr is v\n","        split_data = curr_data[curr_data[bestAttr] == v]\n","        child.count = len(split_data)\n","        root.children.append(child)\n","        # print('splitting on attribute ' + str(bestAttr) + ' for value v= ' + str(v))#-debug\n","        ID3Weather(orig_data, split_data, child, attributesRemaining, classif)\n","    # return the root as the tree\n","    return root\n","\n","\n","\n","\n","# main code for weather example\n","path = basePath + 'weather.csv'\n","df_weather = pd.read_csv(path)\n","attrListWeather = list(df_weather.columns)\n","classif = attrList[-1]\n","classifWeather = attrListWeather[-1]\n","# remove the classification from the attributes list\n","attrListWeather.remove('play')\n","# get the inital entropy for the weather data\n","total_entropy_weather = GetEntropyForAllData(df_weather, classifWeather)\n","rootWeather = Node(None)\n","treeWeather = ID3Weather(df_weather, df_weather, rootWeather, attrListWeather, classifWeather) #-debug with weather data\n","PrintTreeWeather(treeWeather, 0)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["outlook = overcast : yes\t--\t4\n","outlook = rain :\n","\twind = strong : no\t--\t2\n","\twind = weak : yes\t--\t3\n","outlook = sunny :\n","\t\thumidity = high : no\t--\t3\n","\t\thumidity = normal : yes\t--\t2\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"YvYowzzA4vcd"},"source":["##Extra Credit - Learning Curve\n","\n","(05 points)\n","\n","Instead of taking the entire training data (all 800 instances), loop through to select 'x' instances in the increments of 40 (i.e. 40, 80, 120, and so on). For each selected number 'x', randomly pick the example instances from the training data and call the ID3 function to create the decision tree. Calculate the accuracy of the created ID3 tree on the Test data file. Plot the corresponding graph, aka Learning Curve.\n"]},{"cell_type":"code","metadata":{"id":"FYSK99zp5a7H"},"source":["# Loop through to select the number of instances 'x' in increments of 40\n","# For each 'x',\n","    # Randomly select 'x' instances\n","    # Create the ID3 decision tree using those instances\n","    # Calculate the accuracy of the ID3 tree created on the Test data\n","\n","# Plot the learning curve using the accuracy values\n","    # X-axis will be the number of training instances used for creating the tree\n","    # Y-axis will be the accuracy in % on the Test data\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YJSFgNBQrhQU"},"source":["#Submission Instructions\n","\n","1.   Complete all tasks above - **File MUST contain the output for ALL cells**\n","2.   Export this notebook as .ipynb\n","      (File > Download as ipynb)\n","3.   Upload the .ipynb file on Blackboard"]},{"cell_type":"markdown","metadata":{"id":"0lGvLE9H6ptL"},"source":["##Rubric\n","\n","*   (50 points) Gradient Descent Algorithm\n","*   (05 points) Extra Credit - GD Accuracy Plots\n","*   (100 points) ID3 Algorithm\n","*   (05 points) Extra Credit - ID3 Learning Curve\n"]}]}